{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2ForSequenceClassification,GPT2Tokenizer,GPT2Config,get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.functional import F\n",
    "from torch.cuda.amp import autocast as autocast,GradScaler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"..\",\"dataset\",\"data_folder\",\"processed_gcjpy\")\n",
    "model_name = \"../../gpt2\"\n",
    "config = GPT2Config.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name,bos_token = \"<|startoftext|>\",eos_token = \"<|endoftext|>\",pad_token = \"<|pad|>\",cls_token = \"<|cls|>\",sep_token = \"<|sep|>\" ,model_max_length = 1024)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name,num_labels = 66)\n",
    "train_batch_size = 4\n",
    "eval_batch_size = 8\n",
    "lr = 5e-5\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "        metirc = evaluate.load(\"accuracy\")\n",
    "        logits , labels = eval_pred\n",
    "        predictions = np.argmax(logits,axis=-1)\n",
    "        return metirc.compute(predictions=predictions,references=labels)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"],truncation = True) \n",
    "\n",
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, padding=\"max_length\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"csv\", data_files={\"train\":os.path.join(data_path,\"train.csv\"),\"test\":os.path.join(data_path,\"valid.csv\")})\n",
    "tokenized_dataset = datasets.map(tokenize_function,batched=True,remove_columns=\"text\").rename_column(\"label\",\"labels\")\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"],shuffle=True,collate_fn=collate_fn,batch_size = train_batch_size)\n",
    "eval_dataloader = DataLoader(tokenized_dataset[\"test\"] , collate_fn=collate_fn,batch_size = eval_batch_size)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(params=model.parameters(), lr=lr)\n",
    "\n",
    "# Instantiate scheduler\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.06 * (len(train_dataloader) * num_epochs),\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "max_eval_acc = 0\n",
    "iter_to_accumlate = 4\n",
    "epochloss = []\n",
    "trainlogdf = pd.DataFrame(columns=[\"step\",\"trainloss\",\"validloss\",\"acc\",\"f1-score\"])\n",
    "rowindex = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    allloss = 0\n",
    "    for step,batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch.to(device)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss/iter_to_accumlate\n",
    "        loss.backward()\n",
    "        allloss += loss.item()\n",
    "        trainlogdf.loc[rowindex] = [rowindex,loss.item(),None,None,None]\n",
    "        rowindex += 1\n",
    "        epochloss.append(loss.item())\n",
    "        if (step+1)%iter_to_accumlate==0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        if (step+1)%(4*iter_to_accumlate) == 0:\n",
    "            print(\"epoch\",epoch,\"step\",step,\"loss\",loss,sep=\" \")\n",
    "            \n",
    "    print(\"epoch\",epoch,\"trainLoss:\",allloss/(len(train_dataloader)*train_batch_size))\n",
    "\n",
    "    count = 0\n",
    "    model.eval()\n",
    "    validloss = 0\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for step,batch in enumerate(tqdm(eval_dataloader)):\n",
    "        labels += batch['labels'].cpu()\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**batch)\n",
    "        validloss += output.loss.item()\n",
    "        pred = torch.argmax(F.softmax(output.logits.cpu(),dim=1),dim=1)\n",
    "        preds += pred\n",
    "        count += int(sum(batch['labels'].cpu() == pred))\n",
    "    eval_acc = count/132\n",
    "    trainlogdf.loc[rowindex-1,\"validloss\"] = validloss/132\n",
    "    trainlogdf.loc[rowindex-1,\"acc\"] = eval_acc\n",
    "    trainlogdf.loc[rowindex-1,\"f1-score\"] = f1_score(np.array(labels),np.array(preds),average=\"macro\")\n",
    "    print(\"epoch \",epoch,\"acc \",eval_acc)\n",
    "    if eval_acc > max_eval_acc:\n",
    "        max_eval_acc = eval_acc\n",
    "        model.save_pretrained(\"GPT2saved_models\")\n",
    "        torch.save(model.state_dict(),os.path.join(\"checkpoint\",\"model.bin\"))\n",
    "        torch.save(optimizer.state_dict(),os.path.join(\"checkpoint\",\"optimizer.bin\"))\n",
    "        torch.save(lr_scheduler.state_dict(),os.path.join(\"checkpoint\",\"lr_scheduler.bin\"))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlogdf.to_csv(\"trainlog.csv\")\n",
    "tokenizer.save_pretrained(\"GPT2saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2ForSequenceClassification.from_pretrained(\"GPT2saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "model.to(device)\n",
    "model.eval()\n",
    "validloss = 0\n",
    "preds = []\n",
    "labels = []\n",
    "for step,batch in enumerate(tqdm(eval_dataloader)):\n",
    "    labels += batch['labels'].cpu()\n",
    "    batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(**batch)\n",
    "    validloss += output.loss.item()\n",
    "    pred = torch.argmax(F.softmax(output.logits.cpu(),dim=1),dim=1)\n",
    "    print(pred)\n",
    "    print(batch['labels'])\n",
    "    preds += pred\n",
    "    count += int(sum(batch['labels'].cpu() == pred))\n",
    "    print(count)\n",
    "eval_acc = count/(len(eval_dataloader)*eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "cls = pipeline('sentiment-analysis',model=model,tokenizer=tokenizer,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
