{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "对抗训练\n",
    "1. adversarial training\n",
    "2. adversarial fine-tuning\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "from torch.functional import F\n",
    "from transformers import RobertaTokenizer,RobertaForSequenceClassification\n",
    "from transformers import AdamW,get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversari_training(model,tokenizer,train_data_path,valid_data_path,train_batch_size,eval_batch_size,num_epochs,lr,early_stopping,outputdir,trainlogdir,adv_training_data_path,training_or_finetuning = \"training\"):\n",
    "    '''\n",
    "    当training_or_finetuning == \"fine-tuning\"时,进行adversarial fine-tuning\n",
    "    否则,进行adversarial training\n",
    "    '''\n",
    "    traindataset = load_dataset(\"csv\",data_files=train_data_path)[\"train\"]\n",
    "    validdataset = load_dataset(\"csv\",data_files=valid_data_path)[\"train\"]\n",
    "    if training_or_finetuning == \"training\":\n",
    "        advdataset = load_dataset(\"csv\",data_files=adv_training_data_path)[\"train\"]\n",
    "        for i in advdataset:\n",
    "            traindataset = traindataset.add_item(i)\n",
    "            \n",
    "    elif training_or_finetuning == \"fine-tuning\":\n",
    "        advdataset = load_dataset(\"csv\",data_files=adv_training_data_path)[\"train\"]\n",
    "        advdataset = advdataset.map(tokenize_function,batched=True,remove_columns=[\"text\"]).rename_column(\"label\",\"labels\")\n",
    "        adv_dataloader = DataLoader(advdataset , collate_fn=collate_fn,batch_size = train_batch_size)\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"],truncation = True,padding=True) \n",
    "    def collate_fn(examples):\n",
    "        return tokenizer.pad(examples, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    train_tokenized_dataset = traindataset.map(tokenize_function,batched=True,remove_columns=[\"text\"]).rename_column(\"label\",\"labels\")\n",
    "    valid_tokenized_dataset = validdataset.map(tokenize_function,batched=True,remove_columns=[\"text\"]).rename_column(\"label\",\"labels\")\n",
    "    train_dataloader = DataLoader(train_tokenized_dataset,shuffle=True,collate_fn=collate_fn,batch_size = train_batch_size)\n",
    "    eval_dataloader = DataLoader(valid_tokenized_dataset , collate_fn=collate_fn,batch_size = eval_batch_size)\n",
    "        \n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    trainlogdf = pd.DataFrame(columns=[\"step\",\"trainloss\",\"validloss\",\"acc\",\"f1-score\"])\n",
    "    rowindex = 0\n",
    "    eval_no_progress_count = 0\n",
    "    epochloss = []\n",
    "    max_eval_acc = 0\n",
    "    optimizer = AdamW(params=model.parameters(), lr=lr)\n",
    "    iter_to_accumlate = 4\n",
    "    # Instantiate scheduler\n",
    "\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0.06 * (len(train_dataloader) * num_epochs),\n",
    "        num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        allloss = 0\n",
    "        for step,batch in enumerate(tqdm(train_dataloader)):\n",
    "            batch.to(device)\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss/iter_to_accumlate\n",
    "            loss.backward()\n",
    "            allloss += loss.item()\n",
    "            trainlogdf.loc[rowindex] = [rowindex,loss.item(),None,None,None]\n",
    "            rowindex += 1\n",
    "            epochloss.append(loss.item())\n",
    "            if (step+1)%iter_to_accumlate==0:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            if (step+1)%(10*iter_to_accumlate) == 0:\n",
    "                print(\"epoch\",epoch,\"step\",step,\"loss\",loss,sep=\" \")\n",
    "        print(\"epoch\",epoch,\"step\",step,\"trainLoss:\",allloss/(len(train_dataloader)*train_batch_size))\n",
    "        \n",
    "        count = 0\n",
    "        model.eval()\n",
    "        validloss = 0\n",
    "        preds = []\n",
    "        labels = []\n",
    "        for evalstep,batch in enumerate(tqdm(eval_dataloader)):\n",
    "            labels += batch['labels'].cpu()\n",
    "            batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(**batch)\n",
    "            validloss += output.loss.item()\n",
    "            pred = torch.argmax(F.softmax(output.logits.cpu(),dim=1),dim=1)\n",
    "            preds += pred\n",
    "            count += int(sum(batch['labels'].cpu() == pred))\n",
    "        model.train()\n",
    "        eval_acc = count/132\n",
    "        trainlogdf.loc[rowindex-1,\"validloss\"] = validloss/132\n",
    "        trainlogdf.loc[rowindex-1,\"acc\"] = eval_acc\n",
    "        trainlogdf.loc[rowindex-1,\"f1-score\"] = f1_score(np.array(labels),np.array(preds),average=\"macro\")\n",
    "        print(\"epoch \",epoch,\"step\",step,\"acc \",eval_acc)\n",
    "        if eval_acc < max_eval_acc:\n",
    "            eval_no_progress_count += 1\n",
    "            if eval_no_progress_count >=early_stopping:\n",
    "                print(\"Early Stopping:Epoch\",epoch,\" Step\",step,\"Eval_acc\",eval_acc,sep=\" \")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Early Stopping record count\",eval_no_progress_count,\"Max eval acc\",max_eval_acc,sep=\" \")\n",
    "        if eval_acc > max_eval_acc:\n",
    "            if training_or_finetuning == \"fine-tuning\":\n",
    "                '''\n",
    "                加载对抗样本进行训练\n",
    "                '''\n",
    "                model.train()\n",
    "                print(\"adver fine-tuning\")\n",
    "                allloss = 0\n",
    "                for step,batch in tqdm(enumerate(adv_dataloader)):\n",
    "                    batch.to(device)\n",
    "                    output = model(**batch)\n",
    "                    loss = output.loss\n",
    "                    loss.backward()\n",
    "                    allloss += loss.item()\n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                print(\"adver fine-tuning loss\",allloss/(len(adv_dataloader)*train_batch_size))\n",
    "                \n",
    "            max_eval_acc = eval_acc\n",
    "            print(\"Update Max eval acc\",max_eval_acc)\n",
    "            eval_no_progress_count = 0\n",
    "            model.save_pretrained(outputdir)\n",
    "            torch.save(model.state_dict(),os.path.join(\"checkpoint\",\"model.bin\"))\n",
    "            torch.save(optimizer.state_dict(),os.path.join(\"checkpoint\",\"optimizer.bin\"))\n",
    "            torch.save(lr_scheduler.state_dict(),os.path.join(\"checkpoint\",\"lr_scheduler.bin\"))\n",
    "        \n",
    "    trainlogdf.to_csv(trainlogdir)\n",
    "    tokenizer.save_pretrained(outputdir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"microsoft/codebert-base\"\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path,num_labels = 66)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "adversari_training(model,tokenizer,\"../dataset/data_folder/processed_gcjpy/train.csv\",\"../dataset/data_folder/processed_gcjpy/valid.csv\",2,2,30,5e-5,5,\"CODEBERT-ADV-TRAINING\",\"adv-training.log\",\"../dataset/data_folder/processed_gcjpy/adv_training.csv\",\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"microsoft/codebert-base\"\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path,num_labels = 66)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "adversari_training(model,tokenizer,\"../dataset/data_folder/processed_gcjpy/train.csv\",\"../dataset/data_folder/processed_gcjpy/valid.csv\",2,2,30,5e-5,5,\"CODEBERT-ADV-FINE-TUNING\",\"adv-fine-tung.log\",\"../dataset/data_folder/processed_gcjpy/adv_training.csv\",\"fine-tuning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
