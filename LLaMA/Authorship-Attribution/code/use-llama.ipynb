{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ljc/anaconda3/envs/adv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/ljc/anaconda3/envs/adv/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/ljc/anaconda3/envs/adv/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    "    PeftType,\n",
    "    TaskType,\n",
    "    PeftModelForSequenceClassification\n",
    ")\n",
    "from transformers import LlamaTokenizer ,LlamaForSequenceClassification ,LlamaForCausalLM,Trainer,TrainingArguments\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/ljc/.cache/huggingface/datasets/csv/default-360cc5e9d91b8249/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 2/2 [00:00<00:00, 1280.51it/s]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_model = \"../ljcllama-7b-hf\"\n",
    "device_map = \"auto\"\n",
    "datasets = load_dataset(\"csv\", data_files={\"train\":\"train.csv\",\"test\":\"valid.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.17s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32000, 4096, padding_idx=31999)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "        base_model,\n",
    "        num_labels = 66,\n",
    "        load_in_8bit = True,\n",
    "        torch_dtype = torch.float16,\n",
    "        device_map = device_map,\n",
    "        return_dict = True\n",
    "    )\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = -100\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "        metirc = evaluate.load(\"accuracy\")\n",
    "        logits , labels = eval_pred\n",
    "        predictions = np.argmax(logits,axis=-1)\n",
    "        return metirc.compute(predictions=predictions,references=labels)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"],padding = \"max_length\",truncation = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModelForSequenceClassification.from_pretrained(model,\"llama-for-attack-ada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ljc/.cache/huggingface/datasets/csv/default-360cc5e9d91b8249/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ce3b874b0a570f03.arrow\n",
      "Loading cached processed dataset at /home/ljc/.cache/huggingface/datasets/csv/default-360cc5e9d91b8249/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-e718a69fe985bb0d.arrow\n",
      "Loading cached shuffled indices for dataset at /home/ljc/.cache/huggingface/datasets/csv/default-360cc5e9d91b8249/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-d46da5b66211fd10.arrow\n",
      "Loading cached shuffled indices for dataset at /home/ljc/.cache/huggingface/datasets/csv/default-360cc5e9d91b8249/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b6131a171a54aa7d.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = datasets.map(tokenize_function,batched=True)\n",
    "small_train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42)\n",
    "small_eval_dataset = tokenized_dataset[\"test\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [01:12<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007575757575757576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaldataset = tokenized_dataset['test'].remove_columns('text').rename_column('label','labels')\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.functional import F\n",
    "def col_fn(examples):\n",
    "    return tokenizer.pad(examples,padding=\"max_length\",return_tensors=\"pt\")\n",
    "evaldataset = DataLoader(evaldataset,collate_fn=col_fn,batch_size=1)\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Softmax\n",
    "count = 0\n",
    "model.eval()\n",
    "model.to(device)\n",
    "for step,batch in enumerate(tqdm(evaldataset)):\n",
    "    batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(**batch)\n",
    "    count += int(batch['labels']== torch.argmax(F.softmax(output.logits,dim=1),dim=1))\n",
    "print(count/len(evaldataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaldataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m evaldataset:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(batch\u001b[39m.\u001b[39minput_ids)\n\u001b[1;32m      3\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaldataset' is not defined"
     ]
    }
   ],
   "source": [
    "for batch in evaldataset:\n",
    "    print(batch.input_ids)\n",
    "    with torch.no_grad():\n",
    "        output = model(**batch)\n",
    "    label = torch.argmax(Softmax(output.logits).dim,dim=1)\n",
    "    print(label)\n",
    "    print(cls(tokenizer.decode(batch['input_ids'][0],skip_special_tokens=True)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llamapipeline(model,tokenzier):\n",
    "    def cls(example):\n",
    "        input = tokenzier(example,padding=\"max_length\",truncation=True,return_tensors = \"pt\")\n",
    "        print(input)\n",
    "        with torch.no_grad():\n",
    "            output = model(**input)\n",
    "        s = F.softmax(output.logits.float(),dim=1)\n",
    "        print(s)\n",
    "        score,label = torch.max(s,dim=1)\n",
    "        ret = []\n",
    "        for i in range(label.shape[0]):\n",
    "            ret.append({\"label\":int(label[i]),\"score\":float(score[i])})\n",
    "        return ret\n",
    "\n",
    "    return cls    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = llamapipeline(model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 29871, 29896,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n",
      "tensor([[3.1966e-04, 6.1655e-03, 3.5050e-02, 3.5187e-02, 9.4335e-03, 2.9101e-05,\n",
      "         3.7977e-01, 1.1995e-02, 3.1851e-02, 4.6585e-03, 3.5182e-03, 2.0558e-04,\n",
      "         3.2719e-05, 6.8603e-04, 1.8609e-04, 1.3879e-03, 1.3148e-02, 2.7990e-04,\n",
      "         1.0461e-03, 2.6092e-03, 1.9777e-02, 7.2318e-04, 1.0835e-03, 5.9016e-05,\n",
      "         1.4436e-04, 4.7720e-02, 3.3179e-03, 3.5102e-05, 4.2458e-03, 6.9898e-03,\n",
      "         2.6865e-04, 9.3418e-03, 1.1007e-02, 8.1322e-03, 6.3426e-03, 4.7072e-02,\n",
      "         8.2832e-04, 4.6343e-02, 5.7114e-02, 6.3488e-03, 2.3203e-03, 1.5599e-02,\n",
      "         1.6782e-03, 5.5919e-03, 1.2344e-03, 2.1342e-02, 5.9778e-04, 6.9548e-04,\n",
      "         2.4269e-03, 8.6879e-05, 8.1297e-05, 5.2275e-03, 3.0617e-05, 2.8428e-03,\n",
      "         2.1569e-03, 1.3757e-03, 3.6646e-03, 6.4133e-04, 6.4437e-05, 6.8490e-02,\n",
      "         8.0205e-04, 1.3017e-04, 2.0558e-04, 4.3876e-02, 3.3301e-03, 1.0522e-03]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 6, 'score': 0.37977394461631775}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls(\"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForSequenceClassification' is not supported for sentiment-analysis. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "cls2 = pipeline(\"sentiment-analysis\",model=model,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_37', 'score': 0.7037458419799805}]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls2(\"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0023,  0.0010, -0.0005,  ..., -0.0118,  0.0149,  0.0105],\n",
      "        [ 0.0248, -0.0052,  0.0196,  ..., -0.0090, -0.0193, -0.0094],\n",
      "        [ 0.0044, -0.0310, -0.0143,  ..., -0.0125, -0.0143,  0.0019],\n",
      "        ...,\n",
      "        [ 0.0131, -0.0314, -0.0373,  ..., -0.0393, -0.0089, -0.0059],\n",
      "        [ 0.0195, -0.0220, -0.0307,  ..., -0.0327, -0.0201, -0.0332],\n",
      "        [ 0.0171, -0.0031, -0.0016,  ...,  0.0144,  0.0109, -0.0280]],\n",
      "       device='cuda:0', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0023,  0.0010, -0.0005,  ..., -0.0118,  0.0149,  0.0105],\n",
      "        [ 0.0248, -0.0052,  0.0196,  ..., -0.0090, -0.0193, -0.0094],\n",
      "        [ 0.0044, -0.0310, -0.0143,  ..., -0.0125, -0.0143,  0.0019],\n",
      "        ...,\n",
      "        [ 0.0131, -0.0314, -0.0373,  ..., -0.0393, -0.0089, -0.0059],\n",
      "        [ 0.0195, -0.0220, -0.0307,  ..., -0.0327, -0.0201, -0.0332],\n",
      "        [ 0.0171, -0.0031, -0.0016,  ...,  0.0144,  0.0109, -0.0280]],\n",
      "       device='cuda:0', dtype=torch.float16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for line in model.model.score.parameters():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0023,  0.0010, -0.0005,  ..., -0.0118,  0.0149,  0.0105],\n",
      "        [ 0.0248, -0.0052,  0.0196,  ..., -0.0090, -0.0193, -0.0094],\n",
      "        [ 0.0044, -0.0310, -0.0143,  ..., -0.0125, -0.0143,  0.0019],\n",
      "        ...,\n",
      "        [ 0.0131, -0.0314, -0.0373,  ..., -0.0393, -0.0089, -0.0059],\n",
      "        [ 0.0195, -0.0220, -0.0307,  ..., -0.0327, -0.0201, -0.0332],\n",
      "        [ 0.0171, -0.0031, -0.0016,  ...,  0.0144,  0.0109, -0.0280]],\n",
      "       device='cuda:0', dtype=torch.float16, requires_grad=True)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "for line in model.score.parameters():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
